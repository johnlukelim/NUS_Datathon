{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# NUS Datathon CAT A – Team <TEAM NAME>\n",
    "\n",
    "## Problem Overview\n",
    "# Brief description of the task and prediction objective.\n",
    "\n",
    "# Data Cleaning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "df = pd.read_csv(\"sds_datathon_gradsingapore.xlsx - 20251117013849-SurveyExport.csv\")\n",
    "df = df.copy()\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "df.info()\n",
    "df.describe()\n",
    "\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    ")\n",
    "\n",
    "df.isna().sum()\n",
    "# Creating a new column called time_taken to store the values of time_taken to complete the survey\n",
    "df[\"time_taken\"] = \"\" \n",
    "df[\"time_taken\"] = np.nan\n",
    "# We calculate the time taken by first converting the inputs in the date_submitted and time_started \n",
    "# columns to date time format in python and using the formula: time_taken = date_submitted - time_started\n",
    "df[\"date_submitted\"] = pd.to_datetime(df[\"date_submitted\"], errors=\"coerce\")\n",
    "df[\"time_started\"] = pd.to_datetime(df[\"time_started\"], errors=\"coerce\")\n",
    "df[\"time_taken\"] = df[\"date_submitted\"] - df[\"time_started\"]\n",
    "# Now we will convert it to seconds for ease of counting\n",
    "df[\"time_taken_seconds\"] = df[\"time_taken\"].dt.total_seconds()\n",
    "# Drop Unrealistic Values\n",
    "df_cleaned = df[df[\"time_taken_seconds\"] >= 45].copy()\n",
    "\n",
    "# Model building with \"cleaned\" dataset \n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "print(df.info())\n",
    "\n",
    "#Describing each Categorial Variable\n",
    "\n",
    "#status\n",
    "'''\n",
    "sns.countplot(x=\"status\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Status\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Status\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#Country\n",
    "'''\n",
    "sns.countplot(x=\"country\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Country\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Country\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#City\n",
    "'''\n",
    "street_to_city = {\n",
    "    \"Tuas\" : \"Singapore\",\n",
    "    \"Singapore\" : \"Singapore\",\n",
    "    \"Jurong\" : \"Singapore\",\n",
    "    \"Jurong West\" : \"Singapore\",\n",
    "    \"Choa Chu Kang\" : \"Singapore\",\n",
    "    \"Punggol\" : \"Singapore\",\n",
    "    \"Pasir Ris\" : \"Singapore\",\n",
    "    \"Bukit Panjang\" : \"Singapore\",\n",
    "    \"Yishun\" : \"Singapore\",\n",
    "    \"Bukit Batok\" : \"Singapore\",\n",
    "    \"Woodlands\" : \"Singapore\"\n",
    "}\n",
    "\n",
    "df[\"city\"] = df[\"city\"].replace(street_to_city)\n",
    "\n",
    "sns.countplot(x=\"city\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of City\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"City\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#state/region\n",
    "'''\n",
    "sns.countplot(x=\"state/region\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of State/Region\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"State/Region\")\n",
    "plt.show()\n",
    "\n",
    "excluded\n",
    "'''\n",
    "\n",
    "#postal\n",
    "'''\n",
    "sns.countplot(x=\"postal\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Postal\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Postal Code\")\n",
    "plt.show()\n",
    "\n",
    "excluded\n",
    "'''\n",
    "\n",
    "#Higher education\n",
    "'''\n",
    "df[\"which_higher_education_institution_do_you_or_did_you_study_at?\"] = (\n",
    "    df[\"which_higher_education_institution_do_you_or_did_you_study_at?\"]\n",
    "    .str.extract(r\"\\(([^)]+)\\)\")\n",
    ")\n",
    "\n",
    "sns.countplot(x=\"which_higher_education_institution_do_you_or_did_you_study_at?\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Higher Education\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Higher Education\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#Year of study\n",
    "'''\n",
    "year_order = [\"Year 1\", \"Year 2\", \"Year 3\", \"Year 4\", \"Others\"]\n",
    "sns.countplot(x=\"what_is_your_current_year_of_study_as_of_2025?\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df, order=year_order)\n",
    "plt.title(\"Bar Plot Of Year of Study\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Year of Study\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#highest qualification\n",
    "'''\n",
    "sns.countplot(x=\"what_will_be_your_highest_qualification_when_you_graduate?\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Highest Qualification\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Highest Qualification\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#main study subject\n",
    "'''\n",
    "col = \"which_of_the_following_best_describes_the_main_subject_that_you_are_studying?\"\n",
    "df[col] = (\n",
    "    df[col]\n",
    "    .str.replace(r\"\\s*\\(.*?\\)\", \"\", regex=True)\n",
    "    .str.strip()\n",
    "    .str.title()\n",
    ")\n",
    "\n",
    "sns.countplot(x=\"which_of_the_following_best_describes_the_main_subject_that_you_are_studying?\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Main Study Subject\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Main Study Subject\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#Nationality\n",
    "'''\n",
    "sns.countplot(x=\"please_indicate_your_nationality.\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Main Study Subject\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Nationality\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#Gender\n",
    "'''\n",
    "sns.countplot(x=\"what_is_your_gender?\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Gender\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Gender\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#Statements\n",
    "'''\n",
    "sns.countplot(x=\"which_of_these_statements_best_describes_your_current_perception_of_the_organisation_as_an_employer?\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Statements\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Statements\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#type of roles available\n",
    "'''\n",
    "sns.countplot(x=\"types_of_roles_available:what_do_you_wish_to_learn_more_about_regarding_the_organisation_as_an_employer?_(pick_3)\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Types of Roles\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Types of Roles\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#Career progression and development\n",
    "'''\n",
    "sns.countplot(x=\"career_progression_and_development:what_do_you_wish_to_learn_more_about_regarding_the_organisation_as_an_employer?_(pick_3)\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Career Progression and Development\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Career Progression and Development\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#compensation and benefits\n",
    "'''\n",
    "sns.countplot(x=\"compensation_and_benefits:what_do_you_wish_to_learn_more_about_regarding_the_organisation_as_an_employer?_(pick_3)\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Compensation & Benefits\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Compensation & Benefits\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#work life balance\n",
    "'''\n",
    "sns.countplot(x=\"work-life_balance_and_culture:what_do_you_wish_to_learn_more_about_regarding_the_organisation_as_an_employer?_(pick_3)\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Work-life Balance and Culture\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Work-life Balance and Culture\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#application and interview process\n",
    "'''\n",
    "sns.countplot(x=\"application_and_interview_process:what_do_you_wish_to_learn_more_about_regarding_the_organisation_as_an_employer?_(pick_3)\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Application & Interview Process\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Application & Interview Process\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#other write in\n",
    "'''\n",
    "sns.countplot(x=\"other_-_write_in_(required):what_do_you_wish_to_learn_more_about_regarding_the_organisation_as_an_employer?_(pick_3)_ .1\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Other Write In\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Other Write In\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#Motivating Factors\n",
    "'''\n",
    "sns.countplot(x=\"which_of_these_factors_would_most_motivate_you_to_apply_for_a_position_at_the_organisation?\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Motivating Factors\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Motivating Factors\")\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x=\"other_-_write_in_(required):which_of_these_factors_would_most_motivate_you_to_apply_for_a_position_at_the_organisation?\", hue=\"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\" ,data=df)\n",
    "plt.title(\"Bar Plot Of Motivating Factors (Other Write In)\")\n",
    "plt.legend(title=\"Attractiveness Rating\", loc=\"upper right\")\n",
    "plt.xlabel(\"Motivating Factors\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#quantitative variable\n",
    "#Completed + filtered time taken <450\n",
    "'''\n",
    "df_clean = df[(df[\"status\"] == \"Complete\") & (df[\"time_taken_seconds\"] < 450)]\n",
    "\n",
    "sns.boxplot(y='time_taken_seconds', data=df_clean, showfliers=True)\n",
    "plt.title(\"Box Plot of Time Taken in Seconds\")\n",
    "plt.ylabel(\"Time Taken in Seconds\")\n",
    "plt.xlabel(\"Complete\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#Partial + filtered time taken <450\n",
    "'''\n",
    "df_clean = df[(df[\"status\"] == \"Partial\") & (df[\"time_taken_seconds\"] < 450)]\n",
    "\n",
    "sns.boxplot(y='time_taken_seconds', data=df_clean, showfliers=True)\n",
    "plt.title(\"Box Plot of Time Taken in Seconds\")\n",
    "plt.ylabel(\"Time Taken in Seconds\")\n",
    "plt.xlabel(\"Partial\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#Disqualified + filtered time taken <450\n",
    "'''\n",
    "df_filtered = df[(df[\"status\"] == \"Disqualified\") & (df[\"time_taken_seconds\"] < 450)]\n",
    "\n",
    "sns.boxplot(y='time_taken_seconds', data=df_filtered, showfliers=True)\n",
    "plt.title(\"Box Plot of Time Taken in Seconds\")\n",
    "plt.ylabel(\"Time Taken in Seconds\")\n",
    "plt.xlabel(\"Disqualified\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "#====================\n",
    "# Model Building\n",
    "#====================\n",
    "\n",
    "#load data\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "\n",
    "rating_col = \"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\"\n",
    "\n",
    "# Binary target: High (7–10) vs Low (1–6)\n",
    "df[\"high_attractiveness\"] = np.where(df[rating_col] >= 7, 1, 0)\n",
    "\n",
    "y = df[\"high_attractiveness\"]\n",
    "X = df.drop(columns=[rating_col, \"high_attractiveness\"])\n",
    "\n",
    "## Feature Engineering\n",
    "# Numerical features are scaled, while categorical features are one-hot encoded.\n",
    "# Missing values are handled using appropriate imputers or dropped.\n",
    "\n",
    "#identify numeric & categorial columns\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "#pre-processing pipline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "#80-20 train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#logistic regression pre tuning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_prob = log_reg.predict_proba(X_test)[:, 1]\n",
    "roc_log = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "#random forest pre-tuning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "roc_rf = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "#XG boost pre-tuning\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", XGBClassifier(eval_metric=\"logloss\", random_state=42))\n",
    "])\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "y_prob = xgb.predict_proba(X_test)[:, 1]\n",
    "roc_xgb = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "\n",
    "#Naive bayes pre tuning\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", GaussianNB())\n",
    "])\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "y_prob = nb.predict_proba(X_test)[:, 1]\n",
    "roc_nb = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "#KNN pre tuning\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "y_prob = knn.predict_proba(X_test)[:, 1]\n",
    "roc_knn = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "#linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_score = lin_reg.predict(X_test)\n",
    "roc_lin = roc_auc_score(y_test, y_score)\n",
    "\n",
    "#Comparing all ROC-AUC scores\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\n",
    "        \"Logistic Regression\",\n",
    "        \"Random Forest\",\n",
    "        \"XGBoost\",\n",
    "        \"Naive Bayes\",\n",
    "        \"KNN\",\n",
    "        \"Linear Regression\"\n",
    "    ],\n",
    "    \"ROC-AUC\": [\n",
    "        roc_log,\n",
    "        roc_rf,\n",
    "        roc_xgb,\n",
    "        roc_nb,\n",
    "        roc_knn,\n",
    "        roc_lin\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(results.sort_values(\"ROC-AUC\", ascending=False))\n",
    "\n",
    "#tuning for all the models\n",
    "param_grid_log = {\n",
    "    \"model__C\": [0.01, 0.1, 1, 10],\n",
    "    \"model__penalty\": [\"l2\"],\n",
    "    \"model__solver\": [\"lbfgs\"]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"model__n_estimators\": [100, 300],\n",
    "    \"model__max_depth\": [None, 5, 10],\n",
    "    \"model__min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "param_grid_nb = {\n",
    "    \"model__var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    \"model__n_neighbors\": [3, 5, 7, 11],\n",
    "    \"model__weights\": [\"uniform\", \"distance\"],\n",
    "    \"model__metric\": [\"euclidean\", \"manhattan\"]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# =========================\n",
    "# 1. LOAD DATA\n",
    "# =========================\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "\n",
    "# Drop useless column causing warnings\n",
    "df = df.drop(columns=[\"tags\"], errors=\"ignore\")\n",
    "\n",
    "rating_col = \"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\"\n",
    "\n",
    "# Binary target: High (7–10) vs Low (1–6)\n",
    "df[\"high_attractiveness\"] = np.where(df[rating_col] >= 7, 1, 0)\n",
    "\n",
    "y = df[\"high_attractiveness\"]\n",
    "X = df.drop(columns=[rating_col, \"high_attractiveness\"])\n",
    "\n",
    "# =========================\n",
    "# 2. FEATURE TYPES\n",
    "# =========================\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# =========================\n",
    "# 3. PREPROCESSING\n",
    "# =========================\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 4. TRAIN–TEST SPLIT\n",
    "# =========================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 5. MODELS + PARAM GRIDS\n",
    "# =========================\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": (\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        {\n",
    "            \"model__C\": [0.01, 0.1, 1, 10],\n",
    "            \"model__solver\": [\"lbfgs\"]\n",
    "        }\n",
    "    ),\n",
    "\n",
    "    \"Random Forest\": (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            \"model__n_estimators\": [100, 300],\n",
    "            \"model__max_depth\": [None, 5, 10],\n",
    "            \"model__min_samples_split\": [2, 5]\n",
    "        }\n",
    "    ),\n",
    "\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42,\n",
    "            use_label_encoder=False\n",
    "        ),\n",
    "        {\n",
    "            \"model__n_estimators\": [100, 300],\n",
    "            \"model__max_depth\": [3, 6],\n",
    "            \"model__learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    ),\n",
    "\n",
    "    \"Naive Bayes\": (\n",
    "        GaussianNB(),\n",
    "        {\n",
    "            \"model__var_smoothing\": [1e-9, 1e-8, 1e-7]\n",
    "        }\n",
    "    ),\n",
    "\n",
    "    \"KNN\": (\n",
    "        KNeighborsClassifier(),\n",
    "        {\n",
    "            \"model__n_neighbors\": [3, 5, 7, 11],\n",
    "            \"model__weights\": [\"uniform\", \"distance\"]\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 6. HYPERPARAMETER TUNING\n",
    "# =========================\n",
    "results = []\n",
    "\n",
    "for name, (model, param_grid) in models.items():\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        param_grid,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"ROC-AUC\": grid.best_score_\n",
    "    })\n",
    "\n",
    "# =========================\n",
    "# 7. LINEAR REGRESSION (BASELINE)\n",
    "# =========================\n",
    "lin_reg = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_score = lin_reg.predict(X_test)\n",
    "roc_lin = roc_auc_score(y_test, y_score)\n",
    "\n",
    "results.append({\n",
    "    \"Model\": \"Linear Regression (Baseline)\",\n",
    "    \"ROC-AUC\": roc_lin\n",
    "})\n",
    "\n",
    "# =========================\n",
    "# 8. FINAL RESULTS\n",
    "# =========================\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values(\"ROC-AUC\", ascending=False))\n",
    "\n",
    "# ==================================================\n",
    "# MODEL ANALYSIS: CONFUSION MATRIX, FEATURE IMPORTANCE, LR COEFFICIENTS\n",
    "# ==================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, recall_score, f1_score, mean_squared_error, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# ==================================================\n",
    "# 1. LOAD DATA\n",
    "# ==================================================\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "df = df.drop(columns=[\"tags\"], errors=\"ignore\")\n",
    "\n",
    "rating_col = \"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\"\n",
    "\n",
    "df[\"high_attractiveness\"] = np.where(df[rating_col] >= 7, 1, 0)\n",
    "\n",
    "y = df[\"high_attractiveness\"]\n",
    "X = df.drop(columns=[rating_col, \"high_attractiveness\"])\n",
    "\n",
    "# ==================================================\n",
    "# 2. FEATURE TYPES\n",
    "# ==================================================\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# ==================================================\n",
    "# 3. PREPROCESSING\n",
    "# ==================================================\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# ==================================================\n",
    "# 4. TRAIN–TEST SPLIT\n",
    "# ==================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ==================================================\n",
    "# 5. METRICS FUNCTION\n",
    "# ==================================================\n",
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    return {\n",
    "        \"TPR\": tp / (tp + fn),\n",
    "        \"FPR\": fp / (fp + tn),\n",
    "        \"FNR\": fn / (fn + tp),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"F1\": f1_score(y_true, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_prob)),\n",
    "        \"ROC-AUC\": roc_auc_score(y_true, y_prob)\n",
    "    }\n",
    "\n",
    "# ==================================================\n",
    "# 6. MODELS & HYPERPARAMETERS\n",
    "# ==================================================\n",
    "models = {\n",
    "    \"Logistic Regression\": (\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        {\"model__C\": [0.01, 0.1, 1, 10]}\n",
    "    ),\n",
    "    \"Random Forest\": (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            \"model__n_estimators\": [100, 300],\n",
    "            \"model__max_depth\": [None, 5, 10]\n",
    "        }\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42,\n",
    "            use_label_encoder=False\n",
    "        ),\n",
    "        {\n",
    "            \"model__n_estimators\": [100, 300],\n",
    "            \"model__max_depth\": [3, 6],\n",
    "            \"model__learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    ),\n",
    "    \"Naive Bayes\": (\n",
    "        GaussianNB(),\n",
    "        {\"model__var_smoothing\": [1e-9, 1e-8]}\n",
    "    ),\n",
    "    \"KNN\": (\n",
    "        KNeighborsClassifier(),\n",
    "        {\"model__n_neighbors\": [3, 5, 7]}\n",
    "    )\n",
    "}\n",
    "\n",
    "# ==================================================\n",
    "# 7. TRAIN, EVALUATE & STORE METRICS\n",
    "# ==================================================\n",
    "results = []\n",
    "best_estimators = {}  # store best models for later\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        params,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    best_estimators[name] = best_model  # store for feature importance\n",
    "\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    metrics = compute_metrics(y_test, y_prob)\n",
    "    metrics[\"Model\"] = name\n",
    "\n",
    "    results.append(metrics)\n",
    "\n",
    "# ==================================================\n",
    "# 8. LOGISTIC REGRESSION COEFFICIENTS\n",
    "# ==================================================\n",
    "log_model = best_estimators[\"Logistic Regression\"]\n",
    "log_coef = log_model.named_steps[\"model\"].coef_[0]\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "cat_cols = log_model.named_steps[\"preprocessor\"].named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out(categorical_features)\n",
    "all_features = np.concatenate([numeric_features, cat_cols])\n",
    "\n",
    "lr_coef_df = pd.DataFrame({\n",
    "    \"Feature\": all_features,\n",
    "    \"Coefficient\": log_coef\n",
    "}).sort_values(by=\"Coefficient\", key=abs, ascending=False)\n",
    "\n",
    "print(\"\\n=== Top 10 Logistic Regression Coefficients ===\")\n",
    "print(lr_coef_df.head(10))\n",
    "\n",
    "# ==================================================\n",
    "# 9. TREE-BASED FEATURE IMPORTANCE\n",
    "# ==================================================\n",
    "for name in [\"Random Forest\", \"XGBoost\"]:\n",
    "    model = best_estimators[name].named_steps[\"model\"]\n",
    "\n",
    "    # Get all feature names\n",
    "    cat_cols = preprocessor.named_transformers_[\"cat\"][\"onehot\"].get_feature_names_out(categorical_features)\n",
    "    all_features = np.concatenate([numeric_features, cat_cols])\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "    feat_df = pd.DataFrame({\n",
    "        \"Feature\": all_features,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    print(f\"\\n=== Top 10 Features – {name} ===\")\n",
    "    print(feat_df.head(10))\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_df.head(10))\n",
    "    plt.title(f\"Top 10 Feature Importance – {name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==================================================\n",
    "# 10. CONFUSION MATRIX FOR EACH MODEL\n",
    "# ==================================================\n",
    "for name, model in best_estimators.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix – {name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==================================================\n",
    "# METRIC EVALUATION\n",
    "# ==================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    mean_squared_error,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# ==================================================\n",
    "# 1. LOAD DATA\n",
    "# ==================================================\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "df = df.drop(columns=[\"tags\"], errors=\"ignore\")\n",
    "\n",
    "rating_col = \"on_a_scale_from_1_to_10_(1_–_low,_10_–_high),_how_would_you_rate_the_attractiveness_of_the_organisation_as_an_employer?\"\n",
    "\n",
    "df[\"high_attractiveness\"] = np.where(df[rating_col] >= 7, 1, 0)\n",
    "\n",
    "y = df[\"high_attractiveness\"]\n",
    "X = df.drop(columns=[rating_col, \"high_attractiveness\"])\n",
    "\n",
    "# ==================================================\n",
    "# 2. FEATURE TYPES\n",
    "# ==================================================\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "# ==================================================\n",
    "# 3. PREPROCESSING\n",
    "# ==================================================\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, numeric_features),\n",
    "    (\"cat\", categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# ==================================================\n",
    "# 4. TRAIN–TEST SPLIT\n",
    "# ==================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# ==================================================\n",
    "# 5. METRICS FUNCTION\n",
    "# ==================================================\n",
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    return {\n",
    "        \"TPR\": tp / (tp + fn),\n",
    "        \"FPR\": fp / (fp + tn),\n",
    "        \"FNR\": fn / (fn + tp),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"F1\": f1_score(y_true, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_prob)),\n",
    "        \"ROC-AUC\": roc_auc_score(y_true, y_prob)\n",
    "    }\n",
    "\n",
    "# ==================================================\n",
    "# 6. MODELS & HYPERPARAMETERS\n",
    "# ==================================================\n",
    "models = {\n",
    "    \"Logistic Regression\": (\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        {\"model__C\": [0.01, 0.1, 1, 10]}\n",
    "    ),\n",
    "    \"Random Forest\": (\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        {\n",
    "            \"model__n_estimators\": [100, 300],\n",
    "            \"model__max_depth\": [None, 5, 10]\n",
    "        }\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        XGBClassifier(\n",
    "            eval_metric=\"logloss\",\n",
    "            random_state=42,\n",
    "            use_label_encoder=False\n",
    "        ),\n",
    "        {\n",
    "            \"model__n_estimators\": [100, 300],\n",
    "            \"model__max_depth\": [3, 6],\n",
    "            \"model__learning_rate\": [0.05, 0.1]\n",
    "        }\n",
    "    ),\n",
    "    \"Naive Bayes\": (\n",
    "        GaussianNB(),\n",
    "        {\"model__var_smoothing\": [1e-9, 1e-8]}\n",
    "    ),\n",
    "    \"KNN\": (\n",
    "        KNeighborsClassifier(),\n",
    "        {\"model__n_neighbors\": [3, 5, 7]}\n",
    "    )\n",
    "}\n",
    "\n",
    "# ==================================================\n",
    "# 7. TRAIN, EVALUATE & STORE METRICS\n",
    "# ==================================================\n",
    "results = []\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    pipe = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        pipe,\n",
    "        params,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    metrics = compute_metrics(y_test, y_prob)\n",
    "    metrics[\"Model\"] = name\n",
    "\n",
    "    results.append(metrics)\n",
    "\n",
    "# ==================================================\n",
    "# 8. LINEAR REGRESSION (BASELINE)\n",
    "# ==================================================\n",
    "lin_reg = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_score = lin_reg.predict(X_test)\n",
    "\n",
    "lin_metrics = compute_metrics(y_test, y_score)\n",
    "lin_metrics[\"Model\"] = \"Linear Regression\"\n",
    "\n",
    "results.append(lin_metrics)\n",
    "\n",
    "# ==================================================\n",
    "# 9. METRICS TABLE\n",
    "# ==================================================\n",
    "metrics_df = pd.DataFrame(results).set_index(\"Model\")\n",
    "print(\"\\nMODEL PERFORMANCE METRICS\\n\")\n",
    "print(metrics_df.round(3))\n",
    "\n",
    "# ==================================================\n",
    "# 10. BAR PLOT (ROC-AUC)\n",
    "# ==================================================\n",
    "metrics_df[\"ROC-AUC\"].sort_values().plot(\n",
    "    kind=\"barh\", figsize=(8, 5),\n",
    "    title=\"ROC-AUC Comparison Across Models\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"ROC-AUC\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================================================\n",
    "# 11. CALIBRATION CURVE (LOGISTIC REGRESSION)\n",
    "# ==================================================\n",
    "log_model = GridSearchCV(\n",
    "    Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", LogisticRegression(max_iter=1000))\n",
    "    ]),\n",
    "    {\"model__C\": [0.01, 0.1, 1, 10]},\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "log_model.fit(X_train, y_train)\n",
    "y_prob_log = log_model.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_prob_log, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Logistic Regression\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfect Calibration\")\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"Observed Proportion\")\n",
    "plt.title(\"Calibration Curve – Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
